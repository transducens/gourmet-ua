from os.path import join
from enum import Enum

class Backend(Enum):
	MARIAN=0
	FAIRSEQ=1
	PYTORCH_TRANSLATE=3

backend=Backend.MARIAN
if "fairseq" in config:
		if config["fairseq"]:
				backend=Backend.FAIRSEQ

if "pytorch_translate" in config:
		if config["pytorch_translate"]:
				backend=Backend.PYTORCH_TRANSLATE

preprocessTwoDecoders=False
if "preprocessTwoDecoders" in config:
	if config["preprocessTwoDecoders"]:
		preprocessTwoDecoders=True

preprocessFairseqFlag=""
if "preprocessAsync" in config:
	if config["preprocessAsync"]:
		preprocessFairseqFlag=preprocessFairseqFlag+"--disable_bpe_marks"

if "preprocessOnlyFirstSubword" in config:
	if config["preprocessOnlyFirstSubword"]:
		preprocessFairseqFlag=preprocessFairseqFlag+" --async_tags_surface_feed_first"

if "preprocessOnlyLastSubword" in config:
	if config["preprocessOnlyLastSubword"]:
		preprocessFairseqFlag=preprocessFairseqFlag+" --async_tags_surface_feed_last"

if "preprocessAddWait" in config:
	if config["preprocessAddWait"]:
		preprocessFairseqFlag=preprocessFairseqFlag+" --add_wait_action"

charMTpreproc="cat -"
charMTpostproc="cat -"
if "charMT" in config and config["charMT"]:
	charMTpreproc="sed -r 's:([ ]|$):￮\\1:g'"
	charMTpostproc="sed 's:￮::g'"

externalRetained=""
if "externalRetained" in config:
	externalRetained=config["externalRetained"]

#Output dirs
permanentDir = config["permanentDir"]
corpusDir = permanentDir + "/corpus"
evalDir = permanentDir + "/eval"
modelDir = permanentDir + "/model"

#Software dependencies
marian=config["marianDir"]
moses=config["mosesDir"]
subword_nmt=config["subwordNmtDir"]
bpeOperations=config["bpeOperations"]
tools=config["toolsDir"]
smartSegmentation=config["smartSegmentationDir"]

#Multisource
ssuffix=None
if "ssuffix" in config:
    ssuffix=config["ssuffix"]
wwMark="gourmetwwtranslation"
if "wwMark" in config:
    wwMark=config["wwMark"]
swapInputs=False
if "swapInputs" in config:
    if config["swapInputs"]:
        swapInputs=True

#Interleaving
interleaving_tl=False
analyzer_tl=None
if "analyzer_tl" in config:
    interleaving_tl=True
    analyzer_tl=config["analyzer_tl"]

interleaving_sl=False
analyzer_sl=False
if "analyzer_sl" in config:
    interleaving_sl=True
    analyzer_sl=config["analyzer_sl"]

if interleaving_sl and preprocessTwoDecoders:
	preprocessFairseqFlag=preprocessFairseqFlag+" --source-factors"

LANG1=config["lang1"]
LANG2=config["lang2"]

multilangs=[LANG1,LANG2]
multilangs_test=[LANG1]
if ssuffix:
    if swapInputs:
        multilangs=[LANG2+".2s",LANG1,LANG2]
        multilangs_test=[LANG2+".2s",LANG1]
    else:
        multilangs=[LANG1,LANG2+".2s",LANG2]
        multilangs_test=[LANG1,LANG2+".2s"]

#Precomputed BPE, vocab, truecasers
externalVocab=""
if "externalVocab" in config:
    externalVocab=config["externalVocab"]
externalBpe=""
if "bpeModel" in config:
    externalBpe=config["bpeModel"]

externalTruecasers={LANG1:"", LANG2:""}
if "externalTruecasers" in config:
    externalTruecasers=config["externalTruecasers"]

#Mark for multilingual MT
addMark={LANG1:"", LANG2:"", LANG2+"."+str(ssuffix):""}
if "addMark" in config:
    addMark=config["addMark"]

#Morphological segmentation
segModel={LANG1:"", LANG2:""}
if "segModel" in config:
    segModel=config["segModel"]

segParams=""
if "segParams" in config:
    segParams=config["segParams"]

bpeVocThreshold=1
if "bpeVocThreshold" in config:
	bpeVocThreshold=config["bpeVocThreshold"]

maxLegthAfterBpe=100
if "maxLegthAfterBpe" in config:
	maxLegthAfterBpe=config["maxLegthAfterBpe"]


tokOptions="-a -no-escape"
#if interleaving_sl or interleaving_tl:
#    tokOptions="-a -no-escape"
tokenizer_l1 = moses + "/scripts/tokenizer/normalize-punctuation.perl -l " + LANG1 + " | " + moses + "/scripts/tokenizer/tokenizer.perl "+tokOptions+" -l " + LANG1
tokenizer_l2 = moses + "/scripts/tokenizer/normalize-punctuation.perl -l " + LANG2 + " | " + moses + "/scripts/tokenizer/tokenizer.perl "+tokOptions+" -l " + LANG2
detokenizer = moses + "/scripts/tokenizer/detokenizer.perl -l " + LANG2

minLength=1
if "minLength" in config:
	minLength=config["minLength"]

sentenceSplitter1 = moses + "/scripts/ems/support/split-sentences.perl -b -l " + LANG1
sentenceSplitter2 = moses + "/scripts/ems/support/split-sentences.perl -b -l " + LANG2

marianModelFile="model.npz.best-translation.npz.decoder.yml"
trainArgs=""
if "marianArgs" in config:
		trainArgs=config["marianArgs"]
if "trainArgs" in config:
		trainArgs=config["trainArgs"]

trainArgs2=None
if "trainArgs2" in config:
	trainArgs2=config["trainArgs2"]
trainCmd2=""

trainArgs3=None
if "trainArgs3" in config:
	trainArgs3=config["trainArgs3"]
trainCmd3=""


interTrainingCmd="echo 'Nothing'"
if 'interTrainingCmd' in config:
	interTrainingCmd=config['interTrainingCmd']

selectBestCheckpointTwoStep="select-checkpoint-best-tag-perplexity.sh"
if 'twoStepSelectBestSurface' in config and config['twoStepSelectBestSurface']:
	selectBestCheckpointTwoStep="select-best-checkpoint-best-sf-perplexity.sh"

gpuIdCuda=config["gpuId"].replace(" ",",") if isinstance(config["gpuId"], str) else str(config["gpuId"]).replace(" ",",")
gpuId=config["gpuId"]
translateArgs=""
if "translateArgs" in config:
	translateArgs=config["translateArgs"]

#NMT commands
if backend == Backend.MARIAN:
		trainCmd = "{0}/build/marian -d {1} ".format(marian, config["gpuId"]) + " ".join(trainArgs) + " --valid-log " + permanentDir +"/valid.log --log " + permanentDir + "/train.log"
if backend == Backend.FAIRSEQ:
		trainCmd = "CUDA_VISIBLE_DEVICES={}  python3 {}/scripts/train.py ".format(gpuIdCuda,tools) + " ".join(trainArgs)
		if trainArgs2 != None:
				trainCmd2 ="CUDA_VISIBLE_DEVICES={} python3 {}/scripts/train.py ".format(gpuIdCuda,tools) + " ".join(trainArgs2)
				if trainArgs3 != None:
					trainCmd3 ="CUDA_VISIBLE_DEVICES={} python3 {}/scripts/train.py ".format(gpuIdCuda,tools) + " ".join(trainArgs3)
if backend == Backend.PYTORCH_TRANSLATE:
		trainCmd = "CUDA_VISIBLE_DEVICES={}  python3 {}/pytorch_translate/train.py --source-lang {} --target-lang {} ".format(gpuIdCuda,tools,LANG1,LANG2) + " ".join(trainArgs)

if backend == Backend.MARIAN:
		translateCmd = "{0}/build/marian-decoder -d {1} --quiet-translation".format(marian, config["gpuId"])
if backend == Backend.FAIRSEQ:
		translateCmd = "CUDA_VISIBLE_DEVICES={} python3 {}/scripts/interactive.py {}".format(gpuIdCuda,tools, translateArgs)
		validateScript=config["validateScript"]
if backend == Backend.PYTORCH_TRANSLATE:
		translateCmd = "CUDA_VISIBLE_DEVICES={} python3 {}/pytorch_translate/generate.py {}".format(gpuIdCuda,tools, translateArgs)



bleuCmd = moses + "/scripts/generic/multi-bleu-detok.perl"
chrfCmd = "python3 "+tools + "/chrF/chrF++.py"

#Input data prefixes
trainPrefixes=config["initCorpusTrainPrefix"]
devPrefix=config["initCorpusDevPrefix"]
testPrefixes=config["initCorpusTestPrefix"]
assert(len(devPrefix) == 1)

############################################# EVALUATION #############################################################

def allTestNames(dataset):
    names = []
    for f in dataset:
        names.append(os.path.basename(f))
    return names

rule report:
    input:
        expand(evalDir+"/{name}.{metric}", name=allTestNames(testPrefixes), metric=["bleu","chrf"])
    output:
        evalDir+"/report"
    run:
        with open(output[0], "wt") as outHandle:
            for file in input:
                with open(file, "rt") as inHandle:
                    str = inHandle.read()
                    outHandle.write(os.path.basename(file))
                    outHandle.write("\t")
                    outHandle.write(str)

rule multibleu:
    input:
        trans=evalDir+"/{name}.output.detokenized"
        ,
        ref=corpusDir+"/{name}."+"{lang}".format(lang=LANG2)
    output:
        evalDir+"/{name}.bleu"
    shell:
        "cat {input.trans} | {bleuCmd} {input.ref} > {output}"

rule chrf:
    input:
        trans=evalDir+"/{name}.output.detokenized"
        ,
        ref=corpusDir+"/{name}."+"{lang}".format(lang=LANG2)
    output:
        evalDir+"/{name}.chrf"
    shell:
        "{chrfCmd} -H {input.trans}  -R {input.ref} > {output}"

rule translate_only:
    input:
        trans=evalDir+"/in.output.detokenized"

    output:
        evalDir+"/output"
    shell:
      	#"hostname;"
        "cp {input} {output}"

##########################################  RUNNING MT ENGINE ##########################################


if backend == Backend.MARIAN:
    rule translate_test:
        input:
            modelPrevious="{dir}/marian/model.npz.decoder.yml".format(dir=modelDir)
            ,
            test= expand("{corpusDir}/{{name}}.bpe.{lang}",corpusDir=corpusDir,lang=multilangs_test)
        output:
            evalDir+"/{name}.output"
        params:
            model="{dir}/marian/{modelFile}".format(dir=modelDir, modelFile=marianModelFile)

        shell:
            "{translateCmd} -i {input.test} -c {params.model} > {output}"

if backend == Backend.FAIRSEQ:
    rule translate_test_fairseq:
        input:
             model="{}/checkpoints/checkpoint_best_metric.pt".format(modelDir),
             test= expand("{corpusDir}/{{name}}.bpe.{lang}",corpusDir=corpusDir,lang=multilangs_test),
             databin=modelDir+"/data-bin"
        output:
             evalDir+"/{name}.output"
        shell:
             "{translateCmd} --input {input.test} --path {input.model} {input.databin} | grep '^H-' | cut -f 3   > {output}"

if backend == Backend.PYTORCH_TRANSLATE:
    rule translate_test_pytorch:
        input:
             model="{}/checkpoints/averaged_checkpoint_best.pt".format(modelDir),
             test= expand("{corpusDir}/{{name}}.bpe.{lang}",corpusDir=corpusDir,lang=multilangs_test),
        output:
             evalDir+"/{name}.output"
        shell:
             "{translateCmd} --source-text-file {input.test} --target-text-file {input.test} --path {input.model} --source-vocab-file {modelDir}/checkpoints/dictionary-{LANG1}.txt --target-vocab-file {modelDir}/checkpoints/dictionary-{LANG2}.txt --translation-output-file {output} --translation-probs-file {output}.probs"

rule train_nmt:
    input:
        vocab=modelDir+"/vocab."+LANG1+LANG2+".yml",
        train=expand("{corpusDir}/train.clean-bpe.{lang}",corpusDir=corpusDir,lang=multilangs),
        valid=expand("{corpusDir}/dev.bpe.{lang}",corpusDir=corpusDir,lang=multilangs)
    output:
        "{dir}/marian/model.npz.decoder.yml".format(dir=modelDir)
    params:
        additionalVocab= lambda wildcards,input:  input.vocab if ssuffix else ""
    shell:
        "mkdir -p {modelDir}/marian ;{trainCmd} -t {input.train} --valid-sets {input.valid} --vocabs {input.vocab} {input.vocab} {params.additionalVocab} -m {modelDir}/marian/model.npz"

rule train_nmt_pytorch:
    input:
        train=expand("{corpusDir}/train.clean-bpe.{lang}",corpusDir=corpusDir,lang=multilangs),
        valid=expand("{corpusDir}/dev.bpe.{lang}",corpusDir=corpusDir,lang=multilangs)
    output:
        "{dir}/checkpoints/averaged_checkpoint_best.pt".format(dir=modelDir)
    shell:
        "mkdir -p {modelDir}/checkpoints;{trainCmd} --train-source-text-file {corpusDir}/train.clean-bpe.{LANG1} --train-target-text-file {corpusDir}/train.clean-bpe.{LANG2} --eval-source-text-file {corpusDir}/dev.bpe.{LANG1} --eval-target-text-file {corpusDir}/dev.bpe.{LANG2}  --save-dir {modelDir}/checkpoints | tee {modelDir}/train.log "

rule train_nmt_fairseq:
		input:
				databin=modelDir+"/data-bin"
		output:
				"{dir}/checkpoints/checkpoint_best.pt".format(dir=modelDir)
		shell:
				"if [ ! -f \"{modelDir}/checkpoints/checkpoint_last.pt\" -o \"{trainCmd2}\" == \"\" ]; then {trainCmd} --save-dir {modelDir}/checkpoints {input.databin} &> {modelDir}/train.log; fi; if [ \"{trainCmd2}\" != \"\" ]; then  bash {tools}/scripts/{selectBestCheckpointTwoStep} {modelDir}/checkpoints  ; {interTrainingCmd}; {trainCmd2} --save-dir {modelDir}/checkpoints {input.databin} &> {modelDir}/train2.log ; if [  \"{trainCmd3}\" != \"\"  ]; then  {trainCmd3} --save-dir {modelDir}/checkpoints {input.databin} &> {modelDir}/train3.log ; fi ; fi"

rule choose_best_checkpoint:
		input:
				"{dir}/checkpoints/checkpoint_best.pt".format(dir=modelDir)
		output:
				"{dir}/checkpoints/checkpoint_best_metric.pt".format(dir=modelDir)
		shell:
				"bash {tools}/scripts/select-best-checkpoint-fairseq.sh \"{modelDir}/checkpoints\" \"{corpusDir}/dev.bpe.{LANG1}\" \"{modelDir}/data-bin\" \"{validateScript}\" \"{translateArgs}\" \"{gpuIdCuda}\""

################################################## MARIAN VOCAB ################################################################

ruleorder: make_vocab_yml_from_external > make_vocab_yml

rule make_vocab_yml:
    input:
        expand("{corpusDir}/train.clean-bpe.{lang}",corpusDir=corpusDir,lang=multilangs)
    output:
        modelDir+"/vocab.{lang1}{lang2}.yml".format(lang1=LANG1,lang2=LANG2)
    shell:
        "cat {input} | {marian}/build/marian-vocab  > {output}"

rule make_vocab_yml_from_external:
    input:
        externalVocab
    output:
        modelDir+"/vocab.{lang1}{lang2}.yml".format(lang1=LANG1,lang2=LANG2)
    shell:
        "cat {input} > {output}"

rule make_databin_fairseq:
   input:
        train=expand("{corpusDir}/train.clean-bpe.{lang}",corpusDir=corpusDir,lang=multilangs),
        dev=expand("{corpusDir}/dev.bpe.{lang}",corpusDir=corpusDir,lang=multilangs)
   output:
         directory(modelDir+"/data-bin")
   shell:
        "fairseq-preprocess --joined-dictionary  -s {LANG1} -t {LANG2}  --trainpref {corpusDir}/train.clean-bpe --validpref {corpusDir}/dev.bpe --destdir {output} --workers 16 " if not preprocessTwoDecoders else " python3 {tools}/scripts/fairseq_preprocess_factors.py --joined-dictionary  -s {LANG1} -t {LANG2}  --trainpref {corpusDir}/train.clean-bpe --validpref {corpusDir}/dev.bpe --destdir {output} --workers 16 --additional_decoder_tl {preprocessFairseqFlag}"

####################################################### TRUECASE ###########################################################

ruleorder: apply_truecaser_2s > apply_truecaser

rule apply_truecaser:
    input:
        file='{name}.tok.{lang}'
        ,
        model="{dir}/truecaser/".format(dir=modelDir)+"truecase-model.{lang}"
    output:
        '{name}.tc.{lang}'
    shell:
        "cat {input.file} | {moses}/scripts/recaser/truecase.perl -model {input.model} > {output}"

rule apply_truecaser_2s:
    input:
        file='{name}.tok.{lang}.2s'
        ,
        model="{dir}/truecaser/".format(dir=modelDir)+"truecase-model.{lang}"
    output:
        '{name}.tc.{lang}.2s'
    shell:
        "cat {input.file} | {moses}/scripts/recaser/truecase.perl -model {input.model} > {output}"

ruleorder: apply_truecaser_train_2s > apply_truecaser_train

rule apply_truecaser_train:
    input:
        file='{name}.clean.{lang}'
        ,
        model="{dir}/truecaser/".format(dir=modelDir)+"truecase-model.{lang}"
    output:
        '{name}.clean-tc.{lang}'
    shell:
        "cat {input.file} | {moses}/scripts/recaser/truecase.perl -model {input.model} > {output}"

rule apply_truecaser_train_2s:
    input:
        file='{name}.clean.{lang}.2s'
        ,
        model="{dir}/truecaser/".format(dir=modelDir)+"truecase-model.{lang}"
    output:
        '{name}.clean-tc.{lang}.2s'
    shell:
        "cat {input.file} | {moses}/scripts/recaser/truecase.perl -model {input.model} > {output}"

ruleorder: learn_truecaser_external > learn_truecaser

rule learn_truecaser:
    input:
        corpusDir+"/train.clean.{lang}"
    output:
        "{dir}/truecaser/".format(dir=modelDir)+"truecase-model.{lang}"
    shell:
        "mkdir -p {modelDir}/truecaser;"
        "{moses}/scripts/recaser/train-truecaser.perl -corpus {input} -model {output}"

rule learn_truecaser_external:
    input:
        l1=externalTruecasers[LANG1],
        l2=externalTruecasers[LANG2]
    output:
        l1=expand("{dir}/truecaser/truecase-model.{lang}",dir=modelDir,lang=LANG1),
        l2=expand("{dir}/truecaser/truecase-model.{lang}",dir=modelDir,lang=LANG2)
    shell:
        "cp {input.l1} {output.l1}; cp {input.l2} {output.l2}"

####################################################### CLEAN ###########################################################

rule clean:
    input:
        "{pref}.tok."+"{lang1}".format(lang1=LANG1)
        ,
        "{pref}.tok."+"{lang2}".format(lang2=LANG2)
    output:
        "{pref}.clean."+"{lang1}".format(lang1=LANG1)
        ,
        "{pref}.clean."+"{lang2}".format(lang2=LANG2),
        "{pref}.lines-retained"
    shell:
        "{moses}/scripts/training/clean-corpus-n.perl {wildcards.pref}.tok {LANG1} {LANG2} {wildcards.pref}.clean {minLength} 100 {wildcards.pref}.lines-retained"

rule clean_2s:
    input:
        tok="{pref}.tok.{lang}.2s"
        ,
        retained="{pref}.lines-retained"
    output:
        "{pref}.clean.{lang}.2s"
    shell:
        "python3 {tools}/filter-lines-retained.py {input.retained} < {input.tok}  >{output}"

####################################################### TOKENIZE ###########################################################

rule tokenize_file_l1:
    input:
        "{pref}."+"{lang}".format(lang=LANG1)
    output:
        "{pref}.tok."+"{lang}".format(lang=LANG1)
    shell:
        "cat {input} | {tokenizer_l1} > {output}"

rule tokenize_file_l2:
    input:
        "{pref}."+"{lang}".format(lang=LANG2)
    output:
        "{pref}.tok."+"{lang}".format(lang=LANG2)
    shell:
        "cat {input} | {tokenizer_l2} > {output}"

rule tokenize_file_l2_2s:
    input:
        "{pref}."+"{lang}.2s".format(lang=LANG2)
    output:
        "{pref}.tok."+"{lang}.2s".format(lang=LANG2)
    shell:
        "cat {input} | {tokenizer_l2} > {output}"


####################################################### POSTPROCESSING ###########################################################

rule detok:
    input:
        "{pref}.output.detruecased"
    output:
        "{pref}.output.detokenized"
    shell:
        "cat {input}  | {detokenizer} > {output}"

rule detruecase:
    input:
        "{pref}.output.debpe"
    output:
        "{pref}.output.detruecased"
    shell:
        "cat {input} | {{ if [ \"{interleaving_tl}\" == \"True\" ]; then sed 's:interleaved_[^ ]* ::g'  ; else  cat - ; fi }} | {moses}/scripts/recaser/detruecase.perl > {output}"

rule debpe:
    input:
        "{pref}.output"
    output:
        "{pref}.output.debpe"
    shell:
        "cat {input} | sed -r 's/(@@ )|(@@ ?$)//g' | {charMTpostproc} > {output}"

##################################### MORPHOLOGICAL SEGMENTATION ##################

ruleorder: morph_segment_2s > morph_segment

rule morph_segment:
    input:
        '{name}tc.{lang}'
    output:
        '{name}tc.seg.{lang}'
    params:
        sModel=lambda w: segModel[w.lang]
    shell:
        "if [ '{params.sModel}' != ''  ]; then bash {smartSegmentation}/segment.sh -c {input} -m {params.sModel} -l {wildcards.lang} {segParams}  > {output} ; else cp {input} {output}  ;  fi"

rule morph_segment_2s:
    input:
        '{name}tc.{lang}.2s'
    output:
        '{name}tc.seg.{lang}.2s'
    params:
        sModel=lambda w: segModel[w.lang]
    shell:
        "if [ '{params.sModel}' != ''  ]; then bash {smartSegmentation}/segment.sh -c {input} -m {params.sModel} -l {wildcards.lang} {segParams}  > {output} ; else cp {input} {output}  ;  fi"


############################################## BPE ##############################################

rule apply_bpe_test:
    input:
        file="{pref}.tc.seg.{lang}"
        ,
        vocab="{dir}/".format(dir=modelDir)+"vocab.{lang1}{lang2}.bpe".format(lang1=LANG1, lang2=LANG2)
    output:
        "{pref}.bpe.{lang}"
    params:
        mark=lambda w: addMark[w.lang]
    shell:
        "if [ \"{interleaving_sl}\" == \"True\" -a \"{wildcards.lang}\" == \"{LANG1}\" ]; then  {analyzer_sl}  {wildcards.pref}.tok.{wildcards.lang}  {wildcards.pref}.tok.{wildcards.lang}.analyzed ; fi; if [ \"{interleaving_tl}\" == \"True\" -a \"{wildcards.lang}\" == \"{LANG2}\" ]; then  {analyzer_tl} {gpuId}  {wildcards.pref}.tok.{wildcards.lang}  {wildcards.pref}.tok.{wildcards.lang}.analyzed ; fi; cat {input.file} | {charMTpreproc} | {subword_nmt}/subword_nmt/apply_bpe.py --vocabulary {input.vocab}.bpevoc.{wildcards.lang} --vocabulary-threshold {bpeVocThreshold} --glossaries {wwMark}  -c {input.vocab} |  sed 's: ￭:@@ :g' |  {{ if [ \"{params.mark}\" != \"\" ]; then  sed 's:^:{params.mark} :' ; else cat -;  fi  }} | {{ if [ \"{interleaving_sl}\" == \"True\" -a \"{wildcards.lang}\" == \"{LANG1}\" ]; then python3 {tools}/scripts/generate-interleaving-corpus.py  {wildcards.pref}.tok.{wildcards.lang}.analyzed ; else cat - ; fi  }} | {{ if [ \"{interleaving_tl}\" == \"True\" -a \"{wildcards.lang}\" == \"{LANG2}\" ]; then python3 {tools}/scripts/generate-interleaving-corpus.py  {wildcards.pref}.tok.{wildcards.lang}.analyzed ; else cat - ; fi  }} > {output}"

rule apply_bpe_train:
    input:
        file="{pref}.clean-tc.seg.{lang}"
        ,
        vocab="{dir}/".format(dir=modelDir)+"vocab.{lang1}{lang2}.bpe".format(lang1=LANG1, lang2=LANG2)
    output:
        "{pref}.clean-bpe-prelengthfilter.{lang}"
    params:
        mark=lambda w: addMark[w.lang]
    shell:
        "if [ \"{interleaving_sl}\" == \"True\" -a \"{wildcards.lang}\" == \"{LANG1}\" ]; then  {analyzer_sl}  {wildcards.pref}.clean.{wildcards.lang}  {wildcards.pref}.clean.{wildcards.lang}.analyzed ; fi; if [ \"{interleaving_tl}\" == \"True\" -a \"{wildcards.lang}\" == \"{LANG2}\" ]; then if [ ! -f \"{wildcards.pref}.clean.{wildcards.lang}.analyzed\" ]; then {analyzer_tl} {gpuId}  {wildcards.pref}.clean.{wildcards.lang}  {wildcards.pref}.clean.{wildcards.lang}.analyzed; fi ; fi;   cat  {input.file} | {charMTpreproc} | {subword_nmt}/subword_nmt/apply_bpe.py  --vocabulary {input.vocab}.bpevoc.{wildcards.lang} --vocabulary-threshold {bpeVocThreshold} --glossaries {wwMark} -c {input.vocab} | sed 's: ￭:@@ :g' |   {{ if [ \"{params.mark}\" != \"\" ]; then  sed 's:^:{params.mark} :' ; else cat -;  fi  }} | {{ if [ \"{interleaving_sl}\" == \"True\" -a \"{wildcards.lang}\" == \"{LANG1}\" ]; then python3 {tools}/scripts/generate-interleaving-corpus.py  {wildcards.pref}.clean.{wildcards.lang}.analyzed ; else cat - ; fi  }} | {{ if [ \"{interleaving_tl}\" == \"True\" -a \"{wildcards.lang}\" == \"{LANG2}\" ]; then python3 {tools}/scripts/generate-interleaving-corpus.py  {wildcards.pref}.clean.{wildcards.lang}.analyzed  ; else cat - ; fi  }} > {output}"


rule clean_postbpe:
    input:
        "{pref}.clean-bpe-prelengthfilter."+"{lang1}".format(lang1=LANG1)
        ,
        "{pref}.clean-bpe-prelengthfilter."+"{lang2}".format(lang2=LANG2)
    output:
        "{pref}.clean-bpe."+"{lang1}".format(lang1=LANG1)
        ,
        "{pref}.clean-bpe."+"{lang2}".format(lang2=LANG2)
    shell:
        "if [ \"{externalRetained}\" == \"\" ]; then  RETAINED=\"{wildcards.pref}.clean-bpe.lines-retained\" ; else  RETAINED=\"{externalRetained}\" ;fi ;cat {wildcards.pref}.clean-bpe-prelengthfilter.{LANG1} | sed 's:interleaved_[^ ]* ::g' > {wildcards.pref}.clean-bpe-prelengthfilter.nointerl.{LANG1}  ; cat {wildcards.pref}.clean-bpe-prelengthfilter.{LANG2} | sed 's:interleaved_[^ ]* ::g' > {wildcards.pref}.clean-bpe-prelengthfilter.nointerl.{LANG2}; {moses}/scripts/training/clean-corpus-n.perl {wildcards.pref}.clean-bpe-prelengthfilter.nointerl {LANG1} {LANG2} {wildcards.pref}.clean-bpe.nointerl 1 {maxLegthAfterBpe} {wildcards.pref}.clean-bpe.lines-retained ; python3 {tools}/filter-lines-retained.py $RETAINED < {wildcards.pref}.clean-bpe-prelengthfilter.{LANG1}  > {wildcards.pref}.clean-bpe.{LANG1} ; python3 {tools}/filter-lines-retained.py $RETAINED < {wildcards.pref}.clean-bpe-prelengthfilter.{LANG2}  > {wildcards.pref}.clean-bpe.{LANG2}"

ruleorder: learn_bpe_external > learn_bpe

rule learn_bpe:
    input:
        expand("{corpusDir}/train.clean-tc.seg.{lang}",corpusDir=corpusDir,lang=multilangs)
    output:
        modelDir+"/vocab."+"{lang1}{lang2}.bpe".format(lang1=LANG1, lang2=LANG2 )
    shell:
        "for F in {input} ; do cat $F |  sed 's:{wwMark}::g'  | sed -r 's:[ ]+: :g'  > $F.bpeready ; done;  VOCS=""; INS=""; for F in {input} ; do INS=\"$INS $F.bpeready\" ; VOCS=\"$VOCS {output}.bpevoc.${{F##*.}}\" ; done ; {subword_nmt}/subword_nmt/learn_joint_bpe_and_vocab.py --input $INS -s {bpeOperations}  -o {output} --write-vocabulary $VOCS"

rule learn_bpe_external:
    input:
        externalBpe
    output:
        modelDir+"/vocab."+"{lang1}{lang2}.bpe".format(lang1=LANG1, lang2=LANG2 )
    shell:
        "cat {input} > {output}"

###################################### PREPARE DATA ##############################################

rule prepare_traindata:
    input:
         l1=expand("{dataset}.{lang}", dataset=trainPrefixes, lang=LANG1)
         ,
         l2=expand("{dataset}.{lang}", dataset=trainPrefixes, lang=LANG2)
    output:
         l1=corpusDir+"/train.{lang}".format(lang=LANG1)
         ,
         l2=corpusDir+"/train.{lang}".format(lang=LANG2)
    shell:
         "mkdir -p {corpusDir}; cat {input.l1} > {output.l1} && cat {input.l2} > {output.l2}"

rule prepare_traindata_2s:
    input:
         l2=expand("{dataset}.{lang}.{suf}", dataset=trainPrefixes, lang=LANG2,suf=ssuffix)
    output:
         l2=corpusDir+"/train.{lang}.2s".format(lang=LANG2)
    shell:
         "mkdir -p {corpusDir}; cat {input.l2} > {output.l2}"

rule prepare_devdata:
    input:
         l1=expand("{dataset}.{lang}", dataset=devPrefix, lang=LANG1)
         ,
         l2=expand("{dataset}.{lang}", dataset=devPrefix, lang=LANG2)
    output:
         l1=corpusDir+"/dev.{lang}".format(lang=LANG1)
         ,
         l2=corpusDir+"/dev.{lang}".format(lang=LANG2)
    shell:
         "mkdir -p {corpusDir}; cat {input.l1} > {output.l1} && cat {input.l2} > {output.l2}"

rule prepare_devdata_2s:
    input:
         l2=expand("{dataset}.{lang}.{suf}", dataset=devPrefix, lang=LANG2,suf=ssuffix)
    output:
         l2=corpusDir+"/dev.{lang}.2s".format(lang=LANG2)
    shell:
         "mkdir -p {corpusDir}; cat {input.l2} > {output.l2}"


rule prepare_test:
    input:
         l1=expand("{dataset}.{lang}", dataset=testPrefixes, lang=LANG1)
         ,
         l2=expand("{dataset}.{lang}", dataset=testPrefixes, lang=LANG2)
    output:
         expand(corpusDir+"/{name}.{lang}", name=allTestNames(testPrefixes), lang=LANG1)
         ,
         expand(corpusDir+"/{name}.{lang}", name=allTestNames(testPrefixes), lang=LANG2)
    shell:
         "mkdir -p {corpusDir}; cp -rL {input.l1} {input.l2} {corpusDir}"

rule prepare_test_2s:
    input:
         l2=expand("{dataset}.{lang}.{suf}", dataset=testPrefixes, lang=LANG2,suf=ssuffix)
    output:
         expand(corpusDir+"/{name}.{lang}", name=allTestNames(testPrefixes), lang=LANG2+".2s")
    shell:
         "mkdir -p {corpusDir}; cp -r {input.l2} {corpusDir}"
